任务描述

实现一个KNN分类模型，对手写数字集中的数字图片进行分类。

完成距离度量（distance）函数。
完成模型的训练（fit）函数。
完成模型的预测(predict)函数。
相关知识
KNN
KNN（K-Nearest Neighbors）是一种监督学习算法，主要用于分类任务。该算法的核心思想是根据输入数据在特征空间中最相邻的K个对象，通过投票来决定输入数据的分类，可以概括为"近朱者赤，近墨者黑"。

KNN算法通常被认为是“惰性学习”算法，因为它在训练阶段实际上没有显式的训练过程，KNN不需要学习参数或权重，它不对数据进行任何假设或拟合。

距离度量
为了让计算机能够理解和处理图像类型的数据，所有的图片样本都经过预处理转化成了n维向量。两个向量之间的距离代表了这两个样本之间的差异程度，即相似度。

距离越大，表明样本之间的差异越大；反之，距离越小，则意味着两个样本越相似。选择合适的距离或相似性度量取决于具体的应用场景和数据特性。

欧式距离（Euclidean Distance）：欧氏距离是最直观和常用的距离概念，它是两点之间直线距离的平方根，适用于数值型特征，例如物理坐标等。    

d 
Euclidean
​
 (x,y)= 
i=1
∑
n
​
 (x 
i
​
 −y 
i
​
 ) 
2
 
​
 

曼哈顿距离（Manhattan Distance）：曼哈顿距离是指在欧几里得空间中测量沿着坐标轴方向移动的绝对差值之和，适用于计算网格状布局中的距离，例如城市街区的距离。

d 
Manhattan
​
 (x,y)= 
i=1
∑
n
​
 ∣x 
i
​
 −y 
i
​
 ∣

闵可夫斯基距离（Minkowski Distance）：闵可夫斯基距离是欧氏距离和曼哈顿距离的泛化形式，通过调整参数p可以得到不同的距离度量，。

d 
Minkowski
​
 (x,y)=( 
i=1
∑
n
​
 ∣x 
i
​
 −y 
i
​
 ∣ 
p
 ) 
p
1
​
 
 

余弦相似度（Cosine Similarity）：余弦相似度衡量的是两个向量之间的夹角的余弦值，适合于高维空间中的向量比较，例如文本或其他高维稀疏数据。

similarity=cos(θ)= 
∥A∥∥B∥
A⋅B
​
 

Numpy示例
np.sum(x)：计算数组中所有元素的和
np.sqrt(x)：计算数组中每个元素的平方根
np.abs(x)：计算数组中每个元素的绝对值
np.power(x)：计算数组中每个元素的幂
np.dot(x, y)：计算两个数组的点积（内积）
np.linalg.norm(x)：计算数组的范数，通常用于计算向量的长度（或模）
编程要求
根据提示，在右侧编辑器中完善代码，实现一个图片分类的模型，平台会对模型进行评估

测试说明
本关占期末总成绩的2.5分，其中编译通过不返回错误获得0.5分，distance函数测试返回True获得0.5分，模型fit函数测试返回True获得0.5分，模型predict函数测试分类精度≥0.90获得1分。若程序逻辑大致正确但编译未通过获得1分。