任务描述
实现MLP分类模型中，需要使用的激活函数。

完成ReLU函数。
完成Softmax函数。
相关知识
ReLU函数
ReLU函数是一个常用的激活函数，主要用于引入非线性特性。它将所有负值变为0，正值保持不变。其数学公式如下:

ReLU(x)=max(0,x)



Softmax函数
Softmax函数是一种用于多分类问题的激活函数，通常用于神经网络的输出层。它将神经网络的输出转换为概率分布，使所有元素和为1。其数学公式如下:

Softmax(x 
i
​
 )= 
∑ 
j
​
 e 
x 
j
​
 
 
e 
x 
i
​
 
 
​
 

其中 x 
i
​
  是输入向量中的元素。



Cross Entropy Loss函数
交叉熵损失是一个在分类问题中常用的损失函数，用于量化评估模型输出的概率分布与实际标签分布之间的差异。其数学公式如下:

L=− 
i
∑
​
 y 
i
​
 log(p 
i
​
 )

其中y 
i
​
 是实际标签（通常为one-hot编码），p 
i
​
  是预测的概率。

Numpy
np.maximum(X, 0): 该函数返回输入数组 X 和 0 的元素逐个比较后的最大值，结果是一个与 X 形状相同的数组，其中所有负值都被替换为 0。

np.exp(X): 计算输入数组 X 中每个元素的指数（e 的幂次）。

np.max(X, axis=1, keepdims=True)：计算输入数组 X 在指定轴（axis=1，表示沿着行方向）上的最大值。

np.sum(exps, axis=1, keepdims=True)：计算输入数组 exps 在指定轴（axis=1，表示沿着行方向）上的元素和。

np.log(p[range(m), y_true])：计算输入数组 p 中指定索引的元素的自然对数。

编程要求
根据提示，在右侧编辑器中完善代码，实现激活函数，平台会对函数进行评估

测试说明
本关占期末总成绩的2.5分，其中编译通过不返回错误获得0.5分，ReLU函数测试返回True获得1分，Softmax函数测试返回True获得1分。若程序逻辑大致正确但编译未通过获得1分。